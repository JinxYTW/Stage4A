{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFLCXryY1mSP",
        "outputId": "ecf36d7b-6141-4530-8d45-2bd4153da4c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "sOgy3JlW1V14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/data.zip -d /content/custom_data_raw"
      ],
      "metadata": {
        "id": "6ESWroJL1X4A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Répertoire d'origine pour les images et annotations\n",
        "raw_data_path_images = \"/content/custom_data_raw/images\"  # Images sont dans ce dossier\n",
        "raw_data_path_labels = \"/content/custom_data_raw/obj_train_data\"  # Annotations dans ce dossier\n",
        "\n",
        "# Obtenir toutes les images (jpg/png) présentes dans le dossier \"images\"\n",
        "images = [f for f in os.listdir(raw_data_path_images) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "# Split train/val (80% train, 20% val)\n",
        "train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "# Création des dossiers cible pour images et labels\n",
        "base_path = \"/content/custom_data\"\n",
        "\n",
        "\n",
        "os.makedirs(base_path + \"/images/train\", exist_ok=True)\n",
        "os.makedirs(base_path + \"/images/val\", exist_ok=True)\n",
        "os.makedirs(base_path + \"/labels/train\", exist_ok=True)\n",
        "os.makedirs(base_path + \"/labels/val\", exist_ok=True)\n",
        "\n",
        "def move_data(image_list, split):\n",
        "    for img_name in image_list:\n",
        "        # Déplacer l'image vers le bon dossier\n",
        "        src_img = os.path.join(raw_data_path_images, img_name)\n",
        "        dst_img = os.path.join(base_path, f\"images/{split}\", img_name)\n",
        "        shutil.copy(src_img, dst_img)\n",
        "\n",
        "        # Vérifier et déplacer l'annotation .txt correspondante\n",
        "        txt_name = img_name.rsplit('.', 1)[0] + '.txt'\n",
        "        txt_src = os.path.join(raw_data_path_labels, txt_name)\n",
        "        txt_dst = os.path.join(base_path, f\"labels/{split}\", txt_name)\n",
        "\n",
        "        if os.path.exists(txt_src):\n",
        "            shutil.copy(txt_src, txt_dst)\n",
        "        else:\n",
        "            print(f\"Pas d'annotation pour {img_name}\")\n",
        "\n",
        "# Appliquer le déplacement aux deux splits : train et val\n",
        "move_data(train_imgs, \"train\")\n",
        "move_data(val_imgs, \"val\")\n"
      ],
      "metadata": {
        "id": "GF5bKm6B1a8-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "# === PARAMÈTRES ===\n",
        "input_root = \"/content/custom_data\"\n",
        "output_root = \"/content/split_custom_data\"\n",
        "\n",
        "base_split_path = \"/content/split_custom_data\"\n",
        "\n",
        "tile_size = 320\n",
        "overlap = 0\n",
        "\n",
        "splits = [\"train\", \"val\"]\n",
        "\n",
        "def yolo_to_bbox(x_center, y_center, w, h, img_w, img_h):\n",
        "    x1 = int((x_center - w / 2) * img_w)\n",
        "    y1 = int((y_center - h / 2) * img_h)\n",
        "    x2 = int((x_center + w / 2) * img_w)\n",
        "    y2 = int((y_center + h / 2) * img_h)\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def bbox_to_yolo(x1, y1, x2, y2, tile_w, tile_h):\n",
        "    x_center = (x1 + x2) / 2 / tile_w\n",
        "    y_center = (y1 + y2) / 2 / tile_h\n",
        "    w = (x2 - x1) / tile_w\n",
        "    h = (y2 - y1) / tile_h\n",
        "    return x_center, y_center, w, h\n",
        "\n",
        "# Boucle sur train et val\n",
        "for split in splits:\n",
        "    input_img_dir = os.path.join(input_root, f\"images/{split}\")\n",
        "    input_lbl_dir = os.path.join(input_root, f\"labels/{split}\")\n",
        "    output_img_dir = os.path.join(output_root, f\"images/{split}\")\n",
        "    output_lbl_dir = os.path.join(output_root, f\"labels/{split}\")\n",
        "\n",
        "    os.makedirs(output_img_dir, exist_ok=True)\n",
        "    os.makedirs(output_lbl_dir, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_img_dir):\n",
        "        if not filename.lower().endswith(('.jpg', '.png')):\n",
        "            continue\n",
        "\n",
        "        basename = os.path.splitext(filename)[0]\n",
        "        img_path = os.path.join(input_img_dir, filename)\n",
        "        label_path = os.path.join(input_lbl_dir, f\"{basename}.txt\")\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img_h, img_w = img.shape[:2]\n",
        "\n",
        "        annotations = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 5:\n",
        "                        cls, xc, yc, w, h = map(float, parts)\n",
        "                        annotations.append((cls, *yolo_to_bbox(xc, yc, w, h, img_w, img_h)))\n",
        "\n",
        "        step = tile_size - overlap\n",
        "        tile_id = 0\n",
        "\n",
        "        for y in range(0, img_h, step):\n",
        "            for x in range(0, img_w, step):\n",
        "                tile = img[y:y + tile_size, x:x + tile_size]\n",
        "                tile_h, tile_w = tile.shape[:2]\n",
        "\n",
        "                if tile_h < tile_size or tile_w < tile_size:\n",
        "                    continue\n",
        "\n",
        "                tile_filename = f\"{basename}_{tile_id}.jpg\"\n",
        "                label_filename = f\"{basename}_{tile_id}.txt\"\n",
        "                tile_id += 1\n",
        "\n",
        "                cv2.imwrite(os.path.join(output_img_dir, tile_filename), tile)\n",
        "\n",
        "                new_annots = []\n",
        "                for cls, x1, y1, x2, y2 in annotations:\n",
        "                    if x1 >= x + tile_size or x2 <= x or y1 >= y + tile_size or y2 <= y:\n",
        "                        continue\n",
        "\n",
        "                    bx1 = max(0, x1 - x)\n",
        "                    by1 = max(0, y1 - y)\n",
        "                    bx2 = min(tile_size, x2 - x)\n",
        "                    by2 = min(tile_size, y2 - y)\n",
        "\n",
        "                    if bx2 - bx1 < 5 or by2 - by1 < 5:\n",
        "                        continue\n",
        "\n",
        "                    x_c, y_c, w, h = bbox_to_yolo(bx1, by1, bx2, by2, tile_size, tile_size)\n",
        "                    new_annots.append(f\"{int(cls)} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\")\n",
        "\n",
        "                with open(os.path.join(output_lbl_dir, label_filename), \"w\") as f:\n",
        "                    f.write(\"\\n\".join(new_annots))\n",
        "\n",
        "    print(f\"✅ Split terminé pour '{split}' — {len(os.listdir(output_img_dir))} tuiles générées\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP2woMJN1ePJ",
        "outputId": "d6801d1f-e78d-4146-b52d-480b029da79b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Split terminé pour 'train' — 2180 tuiles générées\n",
            "✅ Split terminé pour 'val' — 532 tuiles générées\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin vers le fichier obj.names exporté par CVAT (modifie si besoin)\n",
        "obj_names_path = \"/content/custom_data_raw/obj.names\"\n",
        "\n",
        "# Lire les noms de classes depuis obj.names\n",
        "with open(obj_names_path, 'r') as f:\n",
        "    class_names = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Générer le contenu du fichier YAML avec les augmentations de couleur\n",
        "yaml_content = f\"\"\"\n",
        "path: {base_split_path}\n",
        "train: images/train\n",
        "val: images/val\n",
        "nc: {len(class_names)}\n",
        "names: {class_names}\n",
        "augment: True  # Activer l'augmentation des données\n",
        "auto_augment: albumentations\n",
        "\n",
        "\n",
        "\n",
        "# weights: [0.9, 0.1]  # Exemple de pondération si olive est trop présente\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Écriture du fichier data.yaml\n",
        "with open(f\"{base_path}/data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content.strip())\n",
        "\n",
        "with open(f\"{base_split_path}/data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content.strip())\n",
        "\n",
        "print(\" Fichier data.yaml généré automatiquement :\")\n",
        "print(yaml_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7dld5v1hIg",
        "outputId": "6340d414-853c-4684-d932-6ba960105ffc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fichier data.yaml généré automatiquement :\n",
            "\n",
            "path: /content/split_custom_data\n",
            "train: images/train\n",
            "val: images/val\n",
            "nc: 2\n",
            "names: ['olive', 'potential_olive']\n",
            "augment: True  # Activer l'augmentation des données\n",
            "auto_augment: albumentations\n",
            "\n",
            "\n",
            "\n",
            "# weights: [0.9, 0.1]  # Exemple de pondération si olive est trop présente\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "egOcBT6wy0-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFHnhGgjxfXD",
        "outputId": "9f08288e-4b61-4947-96d3-bac6ec53d0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import torch\n",
        "import psutil\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "W8O3uTGay5Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/train.zip -d /content/train"
      ],
      "metadata": {
        "id": "tE1kpBkW_N2i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/train/train/weights/best.pt\"\n",
        "DATA_YAML = \"/content/split_custom_data/data.yaml\"\n",
        "CONF_THRESHOLD = 0.05\n",
        "CLASS_NAME = \"olive\""
      ],
      "metadata": {
        "id": "HDYIWf83y3p4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading"
      ],
      "metadata": {
        "id": "WWDpUgywy8Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(MODEL_PATH)"
      ],
      "metadata": {
        "id": "pRxzSUdDy9YW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ahmTSHYDzC7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "metrics = model.val(data=DATA_YAML, split=\"val\", conf=CONF_THRESHOLD, verbose=False)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoWQ8Q88zFkA",
        "outputId": "d03155b1-efa8-418c-ca98-5e49457140d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.119 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 92 layers, 25,840,918 parameters, 0 gradients, 78.7 GFLOPs\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 13.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1437.8±550.1 MB/s, size: 52.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/split_custom_data/labels/val... 532 images, 259 backgrounds, 0 corrupt: 100%|██████████| 532/532 [00:00<00:00, 2038.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/split_custom_data/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [04:54<00:00,  8.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        532       1221      0.362      0.361      0.374       0.22\n",
            "Speed: 0.6ms preprocess, 547.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "2whZl34kzGMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import os\n",
        "from ultralytics.utils.metrics import bbox_iou\n",
        "\n",
        "# 🔧 Paramètres\n",
        "model_path = \"/content/train/train/weights/best.pt\"\n",
        "data_path = \"/content/split_custom_data/data.yaml\"\n",
        "val_images_dir = \"/content/split_custom_data/images/val\"\n",
        "conf_threshold = 0.05  # 🔁 MODIFIE ici le seuil de confiance\n",
        "target_class = \"olive\"\n",
        "iou_threshold = 0.5  # pour comptage TP/FN\n",
        "\n",
        "# 📦 Charger modèle + noms des classes\n",
        "model = YOLO(model_path)\n",
        "names = model.names\n",
        "target_id = list(names.values()).index(target_class)\n",
        "\n",
        "# 📂 Liste des fichiers image de validation\n",
        "image_paths = list(Path(val_images_dir).rglob(\"*.jpg\")) + list(Path(val_images_dir).rglob(\"*.png\"))\n",
        "\n",
        "# 🔢 Compteurs\n",
        "TP, FP, FN = 0, 0, 0\n",
        "\n",
        "# 🔍 Parcourir les images\n",
        "import torch\n",
        "\n",
        "# 🔍 Parcourir les images\n",
        "for img_path in image_paths:\n",
        "    results = model.predict(source=str(img_path), conf=conf_threshold, iou=iou_threshold, verbose=False)[0]\n",
        "\n",
        "    # Prédictions pour la classe cible\n",
        "    preds = [b for b in results.boxes.data.cpu().numpy() if int(b[5]) == target_id]\n",
        "    pred_boxes = [b[:4] for b in preds]  # x1, y1, x2, y2\n",
        "\n",
        "    # Ground truth : via les annotations val\n",
        "    label_path = str(img_path).replace(\"/images/\", \"/labels/\").rsplit(\".\", 1)[0] + \".txt\"\n",
        "    gt_boxes = []\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                cls, x, y, w, h = map(float, line.strip().split())\n",
        "                if int(cls) == target_id:\n",
        "                    # YOLO format -> (x1, y1, x2, y2)\n",
        "                    img = cv2.imread(str(img_path))\n",
        "                    H, W = img.shape[:2]\n",
        "                    cx, cy, bw, bh = x * W, y * H, w * W, h * H\n",
        "                    x1 = cx - bw / 2\n",
        "                    y1 = cy - bh / 2\n",
        "                    x2 = cx + bw / 2\n",
        "                    y2 = cy + bh / 2\n",
        "                    gt_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    # 🧠 Matching GT ↔ prédictions\n",
        "    matched_gt = set()\n",
        "    for pred_box in pred_boxes:\n",
        "        matched = False\n",
        "        for i, gt_box in enumerate(gt_boxes):\n",
        "            # Convertir en tensor avant de calculer l'IoU\n",
        "            pred_box_tensor = torch.tensor(pred_box).unsqueeze(0)  # Convertir en tensor\n",
        "            gt_box_tensor = torch.tensor(gt_box).unsqueeze(0)  # Convertir en tensor\n",
        "            iou = bbox_iou(pred_box_tensor, gt_box_tensor)[0]  # Calcul de l'IoU\n",
        "            if iou > iou_threshold and i not in matched_gt:\n",
        "                TP += 1\n",
        "                matched_gt.add(i)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            FP += 1\n",
        "    FN += len(gt_boxes) - len(matched_gt)\n",
        "\n",
        "# 📊 Calcul des métriques\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "print(f\"\\n🔎 Résultats pour la classe '{target_class}' (seuil conf = {conf_threshold}):\")\n",
        "print(f\" - Vrais positifs (TP) : {TP}\")\n",
        "print(f\" - Faux positifs (FP)  : {FP}\")\n",
        "print(f\" - Faux négatifs (FN)  : {FN}\")\n",
        "print(f\" - Précision           : {precision:.3f}\")\n",
        "print(f\" - Rappel              : {recall:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j16RVaRTGDHh",
        "outputId": "354a8c2a-5f82-46ee-95b6-9d199cf300fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Résultats pour la classe 'olive' (seuil conf = 0.05):\n",
            " - Vrais positifs (TP) : 1024\n",
            " - Faux positifs (FP)  : 612\n",
            " - Faux négatifs (FN)  : 170\n",
            " - Précision           : 0.626\n",
            " - Rappel              : 0.858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = model.names\n",
        "class_id = list(names.values()).index(CLASS_NAME)\n",
        "\n",
        "precision = metrics.box.p[class_id]\n",
        "recall = metrics.box.r[class_id]\n",
        "map50 = metrics.box.ap50[class_id]\n",
        "map5095 = metrics.box.ap[class_id]\n",
        "\n",
        "f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "print(f\"\\n🔎 Métriques classe '{CLASS_NAME}' (id {class_id}):\")\n",
        "print(f\" - Précision : {precision:.3f}\")\n",
        "print(f\" - Rappel    : {recall:.3f}\")\n",
        "print(f\" - F1-score  : {f1:.3f}\")\n",
        "print(f\" - mAP@0.5   : {map50:.3f}\")\n",
        "print(f\" - mAP@0.5:0.95 : {map5095:.3f}\")\n",
        "\n",
        "# === TEMPS D'INFÉRENCE (moyen estimé) ===\n",
        "inference_time_total = metrics.speed['inference']  # en ms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === TAILLE DU MODÈLE ===\n",
        "model_size = os.path.getsize(MODEL_PATH) / (1024 ** 2)\n",
        "print(f\"💾 Taille du modèle : {model_size:.2f} MB\")\n",
        "\n",
        "# === MÉMOIRE GPU ou RAM ===\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"📊 Mémoire GPU utilisée : {torch.cuda.max_memory_allocated() / (1024**2):.2f} MB\")\n",
        "else:\n",
        "    print(f\"📊 Mémoire RAM utilisée : {psutil.Process().memory_info().rss / (1024**2):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pWrIVMMzLOH",
        "outputId": "003f7b26-4a10-4428-d91f-3a8ad3f54dfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Métriques classe 'olive' (id 0):\n",
            " - Précision : 0.723\n",
            " - Rappel    : 0.722\n",
            " - F1-score  : 0.723\n",
            " - mAP@0.5   : 0.749\n",
            " - mAP@0.5:0.95 : 0.441\n",
            "💾 Taille du modèle : 49.58 MB\n",
            "📊 Mémoire RAM utilisée : 1342.45 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sahi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "BM8M8c0gpdt8",
        "outputId": "8494a927-af3a-4a81-9695-782d9db36c93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sahi\n",
            "  Downloading sahi-0.11.22-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sahi) (8.1.8)\n",
            "Collecting fire (from sahi)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencv-python<=4.10.0.84 (from sahi)\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (11.2.1)\n",
            "Collecting pybboxes==0.1.6 (from sahi)\n",
            "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from sahi) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sahi) (2.32.3)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (2.1.0)\n",
            "Collecting terminaltables (from sahi)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from sahi) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pybboxes==0.1.6->sahi) (2.0.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->sahi) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (2025.1.31)\n",
            "Downloading sahi-0.11.22-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
            "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9b79db62cd9b495a314ea365f5dc72df4e5019b893eb37fb6c51ab68a961e947\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: terminaltables, pybboxes, opencv-python, fire, sahi\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.11.0.86\n",
            "    Uninstalling opencv-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-4.11.0.86\n",
            "Successfully installed fire-0.7.0 opencv-python-4.10.0.84 pybboxes-0.1.6 sahi-0.11.22 terminaltables-3.1.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              },
              "id": "5af08ae9ceab4e18925d569b808a33d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.models.ultralytics import UltralyticsDetectionModel\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import cv2\n",
        "from ultralytics.utils.metrics import bbox_iou\n",
        "\n",
        "# === PARAMÈTRES ===\n",
        "MODEL_PATH = \"/content/train/train/weights/best.pt\"\n",
        "DATA_YAML = \"/content/split_custom_data/data.yaml\"\n",
        "VAL_IMAGES_DIR = \"/content/split_custom_data/images/val\"\n",
        "CONF_THRESHOLD = 0.05\n",
        "IOU_THRESHOLD = 0.5\n",
        "CLASS_NAME = \"olive\"\n",
        "SLICED_IMAGE_SIZE = 320  # taille des tuiles SAHI\n",
        "OVERLAP_RATIO = 0.2\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Charger modèle SAHI\n",
        "detection_model = UltralyticsDetectionModel(\n",
        "    model_path=MODEL_PATH,\n",
        "    confidence_threshold=CONF_THRESHOLD,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "# Obtenir le class_id de la classe cible\n",
        "model = YOLO(MODEL_PATH)\n",
        "names = model.names\n",
        "class_id = list(names.values()).index(CLASS_NAME)\n",
        "\n",
        "# 🔢 Compteurs\n",
        "TP, FP, FN = 0, 0, 0\n",
        "\n",
        "# 🔍 Parcourir les images de validation\n",
        "image_paths = list(Path(VAL_IMAGES_DIR).rglob(\"*.jpg\")) + list(Path(VAL_IMAGES_DIR).rglob(\"*.png\"))\n",
        "\n",
        "for img_path in image_paths:\n",
        "    # === PRÉDICTION SAHI ===\n",
        "    result = get_sliced_prediction(\n",
        "        str(img_path),\n",
        "        detection_model,\n",
        "        slice_height=SLICED_IMAGE_SIZE,\n",
        "        slice_width=SLICED_IMAGE_SIZE,\n",
        "        overlap_height_ratio=OVERLAP_RATIO,\n",
        "        overlap_width_ratio=OVERLAP_RATIO,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # === Récupérer les boxes SAHI ===\n",
        "    preds = []\n",
        "    for pred in result.object_prediction_list:\n",
        "        if pred.category.id == class_id:\n",
        "            bbox = pred.bbox.to_xyxy()\n",
        "            preds.append(bbox)\n",
        "\n",
        "    # === Ground-truths ===\n",
        "    label_path = str(img_path).replace(\"/images/\", \"/labels/\").rsplit(\".\", 1)[0] + \".txt\"\n",
        "    gt_boxes = []\n",
        "    if os.path.exists(label_path):\n",
        "        img = cv2.imread(str(img_path))\n",
        "        H, W = img.shape[:2]\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                cls, x, y, w, h = map(float, line.strip().split())\n",
        "                if int(cls) == class_id:\n",
        "                    # YOLO format → xyxy\n",
        "                    cx, cy, bw, bh = x * W, y * H, w * W, h * H\n",
        "                    x1 = cx - bw / 2\n",
        "                    y1 = cy - bh / 2\n",
        "                    x2 = cx + bw / 2\n",
        "                    y2 = cy + bh / 2\n",
        "                    gt_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    # === MATCHING IoU ===\n",
        "    matched_gt = set()\n",
        "    for pred_box in preds:\n",
        "        matched = False\n",
        "        for i, gt_box in enumerate(gt_boxes):\n",
        "            pred_tensor = torch.tensor(pred_box).unsqueeze(0)\n",
        "            gt_tensor = torch.tensor(gt_box).unsqueeze(0)\n",
        "            iou = bbox_iou(pred_tensor, gt_tensor)[0]\n",
        "            if iou > IOU_THRESHOLD and i not in matched_gt:\n",
        "                TP += 1\n",
        "                matched_gt.add(i)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            FP += 1\n",
        "    FN += len(gt_boxes) - len(matched_gt)\n",
        "\n",
        "# 📊 Calcul des métriques\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "print(f\"\\n🔎 Résultats pour la classe '{CLASS_NAME}':\")\n",
        "print(f\" - Vrais positifs (TP) : {TP}\")\n",
        "print(f\" - Faux positifs (FP)  : {FP}\")\n",
        "print(f\" - Faux négatifs (FN)  : {FN}\")\n",
        "print(f\" - Précision           : {precision:.3f}\")\n",
        "print(f\" - Rappel              : {recall:.3f}\")\n",
        "print(f\" - F1-score            : {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "rHQb01N8eWDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852d4a41-47a5-4182-c4af-6bf07cc0a6dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Résultats pour la classe 'olive':\n",
            " - Vrais positifs (TP) : 1016\n",
            " - Faux positifs (FP)  : 547\n",
            " - Faux négatifs (FN)  : 178\n",
            " - Précision           : 0.650\n",
            " - Rappel              : 0.851\n",
            " - F1-score            : 0.737\n"
          ]
        }
      ]
    }
  ]
}